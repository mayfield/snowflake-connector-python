#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (c) 2012-2017 Snowflake Computing Inc. All right reserved.
#

import collections
import contextlib
import math
import random
import sys
import threading
import time
import weakref
from concurrent import futures
from logging import getLogger

from .compat import PY2
from .errorcode import ER_CHUNK_DOWNLOAD_FAILED
from .errors import (Error, OperationalError)

DEFAULT_REQUEST_TIMEOUT = 3600

MAX_RETRY_DOWNLOAD = 3
WAIT_TIME_IN_SECONDS = 1200

# Scheduling ramp constants.  Use caution if changing these.
SCHED_MAX_PENDING = 40  # Adjust down to reduce memory usage.
SCHED_GROWTH_ADJ = 1.15  # Increase to ramp concurrency faster.
SCHED_SHRINK_ADJ = 0.98  # Adjusts concurrency down when data is ready.

SSE_C_ALGORITHM = u"x-amz-server-side-encryption-customer-algorithm"
SSE_C_KEY = u"x-amz-server-side-encryption-customer-key"
SSE_C_AES = u"AES256"

logger = getLogger(__name__)
timer = getattr(time, 'perf_counter', time.time)


class DownloaderStats(object):
    u"""
    Organize the stats for chunk downloads so questions about aggregate
    bandwidth can be asked in a reasonable way.
    """

    def __init__(self):
        self._records = []
        self._lock = threading.Lock()

    @contextlib.contextmanager
    def track(self):
        data_out = []
        start = timer()
        yield data_out
        finish = timer()
        record = {
            "started": start,
            "finished": finish,
            "elapsed": finish - start,
            "data": data_out[0]
        }
        with self._lock:
            self._records.append(record)

    def __getitem__(self, rng):
        u"""
        Return the elements falling between the slice arg.  The values for the
        slice should be timestamps generated by `timer`.  Data with a partial
        intersection of the range are prorated.
        """
        assert isinstance(rng, slice), 'Only slices supported'
        assert rng.step is None, 'Step not supported'
        out = []
        start = rng.start if rng.start is None or rng.start >= 0 else \
            timer() + rng.start
        stop = rng.stop if rng.stop is None or rng.stop >= 0 else \
            timer() + rng.stop
        with self._lock:
            for record in self._records:
                if (start is None or start < record['finished']) and \
                   (stop is None or stop > record['started']):
                    window = record['elapsed']
                    adj_record = record.copy()
                    if start is not None and start > record['started']:
                        adj_record['started'] = start
                        window -= start - record['started']
                    if stop is not None and stop < record['finished']:
                        adj_record['finished'] = stop
                        window -= record['finished'] - stop
                    adj_record['elapsed'] = window
                    adj_record['data'] *= window / record['elapsed']
                    out.append(adj_record)
        return out

    def rolling(self, period=None, count=None):
        u"""
        Return the adjusted average for the trailing window of `period`
        seconds.
        """
        if period is not None:
            records = self[-period:]
        else:
            records = self._records[-count:]
        if not records:
            return 0
        first_start = min(x['started'] for x in records)
        last_finish = max(x['finished'] for x in records)
        walltime = last_finish - first_start
        return sum(x['data'] for x in records) / walltime

    def concurrency(self, period=None, count=None):
        u"""
        Return the achieved concurrency average for last `period` seconds.
        """
        if period is not None:
            records = self[-period:]
        else:
            records = self._records[-count:]
        if not records:
            return 0
        first_start = min(x['started'] for x in records)
        last_finish = max(x['finished'] for x in records)
        walltime = last_finish - first_start
        busytime = sum(x['elapsed'] for x in records)
        return busytime / walltime


class SnowflakeChunkDownloader(object):
    u"""
    Large Result set chunk downloader class.
    """

    Status = collections.namedtuple('ChunkDownloaderStatus', 'active, ready')

    def __init__(self, chunks, connection, cursor, qrmk, chunk_headers,
                 use_ijson=False):
        self._use_ijson = use_ijson
        self._connection = connection
        self._cursor = cursor
        self._qrmk = qrmk
        self._headers = chunk_headers
        self._manifests = chunks
        self._total = len(chunks)
        self._calling_thread = None
        self._consumed = 0
        self._sched_lock = threading.RLock()
        self._sched_work = {}
        self._sched_cursor = 0
        self._sched_active = 0
        self._sched_ready = 0
        self._sched_backoff_till = 0
        self._sched_ticks = 0
        self._sched_pending_limit = 1
        self.stats = DownloaderStats()
        self._stats_hist = collections.deque(maxlen=100)
        # Improved performance for high thread counts.
        if not PY2:
            self._switchinterval_save = sys.getswitchinterval()
            sys.setswitchinterval(0.200)

    def __iter__(self):
        return self

    def next(self):
        self.assertFixedThread()
        idx = self._consumed
        if idx >= self._total:
            raise StopIteration()
        with self._sched_lock:
            if idx == self._sched_cursor:
                logger.warning(u'chunk downloader reached starvation')
                self.sched_next()
        for attempt in range(MAX_RETRY_DOWNLOAD + 1):
            if attempt:
                logger.warning(u'retrying chunk %d download (retry %d/%d)',
                               idx + 1, attempt, MAX_RETRY_DOWNLOAD)
                self._sched(idx, retry=attempt)
            with self._sched_lock:
                fut = self._sched_work.pop(idx)
            # Wait for the result with a small inner timeout that is used
            # to maintain the scheduler (maybe_sched_more).  Otherwise it
            # might become starved for some network conditions.
            start_ts = timer()
            while True:
                try:
                    rows = fut.result(timeout=0.250)
                except futures.TimeoutError:
                    elapsed = timer() - start_ts
                    if elapsed > WAIT_TIME_IN_SECONDS:
                        logger.warning(
                            u'chunk %d download timed out after %g second(s)',
                            idx + 1, elapsed)
                        with self._sched_lock:
                            fut.cancel()
                            self._sched_active -= 1
                            break
                    else:
                        self._sched_tick()
                else:
                    self._consumed += 1
                    with self._sched_lock:
                        self._sched_ready -= 1
                    self._sched_tick()
                    return rows
        Error.errorhandler_wrapper(
            self._connection,
            self._cursor,
            OperationalError,
            {
                u'msg': u'The result set chunk download fails or hang for '
                        u'unknown reason.',
                u'errno': ER_CHUNK_DOWNLOAD_FAILED
            })

    __next__ = next

    def get_status(self):
        """ Thread safe access to tuple of (active, ready) chunk counts. """
        with self._sched_lock:
            return self.Status(self._sched_active, self._sched_ready)

    def assertFixedThread(self):
        """
        Ensure threadsafety == 2 is enforced.
        https://www.python.org/dev/peps/pep-0249/#threadsafety
        """
        current = threading.current_thread()
        if self._calling_thread is None:
            self._calling_thread = weakref.ref(current)
        else:
            expected = self._calling_thread()
            assert current is expected, '%r is not %r' % (current, expected)

    def sched_next(self):
        with self._sched_lock:
            idx = self._sched_cursor
            if idx >= self._total:
                return None
            self._sched_chunk(idx)
            self._sched_cursor += 1
            return idx

    def _sched_chunk(self, idx, retry=None):
        """
        Schedule a download in a background thread.  Return a Future object
        that represents the eventual result.
        """
        future = futures.Future()
        with self._sched_lock:
            assert idx not in self._sched_work or \
                   self._sched_work[idx].cancelled()
            self._sched_work[idx] = future
        tname = 'ChunkDownloader_%d' % (idx + 1)
        if retry is not None:
            tname += '_retry_%d' % retry
        t = threading.Thread(name=tname,
                             target=self._fetch_chunk_worker_runner,
                             args=(future, self._manifests[idx]))
        t.daemon = True
        with self._sched_lock:
            self._sched_active += 1
        t.start()
        return future

    def _fetch_chunk_worker_runner(self, future, chunk):
        """
        Entry point for ChunkDownloader threads.  Thread safety rules apply
        from here out.
        """
        try:
            with self.stats.track() as data_out:
                rows = self._fetch_chunk_worker(chunk)
                data_out.append(chunk['compressedSize'])  # XXX always compressed?
        except BaseException as e:
            exc = e
        else:
            exc = None
        with self._sched_lock:
            if future.cancelled():
                if exc is None:
                    logger.warning("Ignoring good result from cancelled work")
                return
            self._sched_active -= 1
            self._sched_ready += 1
            if exc is not None:
                future.set_exception(exc)
            else:
                future.set_result(rows)
        self._sched_tick()

    def _sched_tick(self,
                    _max_pending=SCHED_MAX_PENDING,
                    _growth_adj=SCHED_GROWTH_ADJ,
                    _shrink_adj=SCHED_SHRINK_ADJ):
        """ Threadsafe scheduler for chunk queue management.  This is a variant
        of a hillclimbing algo that tries to hone in on the optimal amount of
        network concurrency.  Each "tick" monitors progress, makes any changes
        to the amount of maximum concurrency and may start new chunk
        downloads. """

        with self._sched_lock:
            if self._sched_cursor >= self._total:
                return
            avgrate = self.stats.rolling(period=60) * 8 / 1024 / 1024
            avgrate2 = self.stats.rolling(count=_max_pending) * 8 / 1024 / 1024
            logger.warn('AVGRATE: 60s=%f 40c=%f', avgrate, avgrate2)
            avgconcur = self.stats.concurrency(period=60)
            avgconcur2 = self.stats.concurrency(count=_max_pending)
            avgrate = avgrate2
            logger.warn('AVGCONCUR: 60s=%f 40c=%f', avgconcur, avgconcur2)
            avgconcur = avgconcur2
            sample = avgrate, avgconcur
            if not self._stats_hist or self._stats_hist[-1] != sample:
                self._stats_hist.append(sample)
            best_rate = max(self._stats_hist)[0]
            accept_rate = 0.8 * best_rate
            accept_concur_min = min(c for rate, c in self._stats_hist
                                    if rate >= accept_rate)
            requests = sum(1 for x in threading.enumerate() if 'ChunkDownloader' in str(x))
            logger.critical("size: %d, activedl: %d, rate: %f mbps, concur: %f, accept_concur_min: %f, best_rate: %f", len(self._stats_hist), requests, avgrate, avgconcur, accept_concur_min, best_rate)
            concur_limit = accept_concur_min + self._sched_ready
            # Trend towards pending limit if it exceeds the concurrency
            # setting.  This is because the cursor is indicating it's starving
            # for data and we may need to find higher upper limits.  The intent
            # is to bounce off the rev limiter until it finds the local
            # maximum.
            if self._sched_pending_limit > concur_limit:
                concur_limit *= 1 + random.random() + 2 * \
                    math.log1p(concur_limit / self._sched_pending_limit)

            self._sched_ticks += 1
            pending = self._sched_active + self._sched_ready
            now = timer()
            # Only perform limit adjustments occasionally and backoff
            # gradually with each use to stabilize the values.
            if now > self._sched_backoff_till:
                self._sched_backoff_till = now + (self._sched_ticks / 100)
                if pending < _max_pending and self._sched_ready <= 1:
                    #adj = 1 + ((1 - min(pending / _max_pending, 0.99)) *
                    #           _growth_adj)
                    adj = _growth_adj
                else:
                    adj = _shrink_adj
                self._sched_pending_limit = min(_max_pending, max(1,
                    self._sched_pending_limit * adj))
            if pending < min(concur_limit, self._sched_pending_limit):
                # Only launch one per tick to avoid bursts.
                logger.info(u'<reverse>sched status, active: %d, ready: %d, '
                            u'pending_limit: %f, concur_limit: %f', self._sched_active,
                            self._sched_ready, self._sched_pending_limit,
                            concur_limit)
                self.sched_next()

    def _fetch_chunk_worker(self, chunk):
        """
        Thread worker to fetch the chunk from S3.
        """
        if self._headers is not None:
            headers = self._headers
        else:
            headers = {}
            if self._qrmk is not None:
                headers[SSE_C_ALGORITHM] = SSE_C_AES
                headers[SSE_C_KEY] = self._qrmk
        timeouts = (
            self._connection._connect_timeout,
            self._connection._connect_timeout,
            DEFAULT_REQUEST_TIMEOUT
        )
        return self._connection.rest.fetch(
            u'get', chunk['url'], headers, timeouts=timeouts,
            is_raw_binary=True, is_raw_binary_iterator=False,
            use_ijson=self._use_ijson)

    def terminate(self):
        """
        Terminates downloading the chunks.
        """
        if not PY2:
            sys.setswitchinterval(self._switchinterval_save)
        with self._sched_lock:
            futures = list(self._sched_work.values())
            self._sched_work = None
        for f in futures:
            f.cancel()

    def __del__(self):
        try:
            self.terminate()
        except:
            pass
