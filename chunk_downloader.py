#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (c) 2012-2017 Snowflake Computing Inc. All right reserved.
#

import contextlib
import json
import math
import random
import threading
import time
import weakref
from concurrent import futures
from logging import getLogger

from .errorcode import ER_CHUNK_DOWNLOAD_FAILED
from .errors import (Error, OperationalError)
from .ssl_wrap_socket import set_proxies

MAX_RETRY_DOWNLOAD = 3
WAIT_TIME_IN_SECONDS = 1200

# Scheduling ramp constants.  Use caution if changing these.
SCHED_ACTIVE_LIMIT = 24  # Adjust down to reduce max memory usage.
SCHED_READY_PCT_LIMIT = 0.8  # Maximum ready/total-pending ratio.
SCHED_RATE_WINDOW_FACTOR = 8  # Adjusts window size for rate est.
SCHED_GROWTH_FACTOR = 0.5  # Adjusts how fast concurrency scales.
SCHED_GROWTH_MIN = 1  # Minimum jump for concurrency growth.
SCHED_RATE_FLOOR_PCT = 0.80
SCHED_CONCUR_CEIL_PCT = 0.70  # When concurrency at best bandwidth is less
                              # than this percentage of the peak concurrency.
SCHED_ACTIVE_IDEAL_INITIAL = 2

SSE_C_ALGORITHM = u"x-amz-server-side-encryption-customer-algorithm"
SSE_C_KEY = u"x-amz-server-side-encryption-customer-key"
SSE_C_AES = u"AES256"

logger = getLogger(__name__)
timer = getattr(time, 'perf_counter', time.time)


class DownloaderStats(object):
    u"""
    Organize the stats for chunk downloads so questions about aggregate
    bandwidth can be asked in a reasonable way.
    """

    def __init__(self, records=None):
        self._records = records or []
        self._lock = threading.Lock()

    @contextlib.contextmanager
    def track(self):
        start = timer()
        data_out = []
        record = {
            "started": start,
            "finished": None,
            "elapsed": None,
            "data": None
        }
        with self._lock:
            self._records.append(record)
        yield data_out
        finish = timer()
        with self._lock:
            record['finished'] = finish
            record['elapsed'] = finish - start
            assert len(data_out) == 1
            record['data'] = data_out[0]

    def __len__(self):
        with self._lock:
            return len(self._records)

    def __getitem__(self, rng):
        u"""
        Return the elements falling between the slice arg.  The values for the
        slice should be timestamps generated by `timer`.  Data with a partial
        intersection of the range are prorated.
        """
        with self._lock:
            return type(self)(self._records[rng])
        assert isinstance(rng, slice), 'Only slices supported'
        assert rng.step is None, 'Step not supported'
        start = rng.start if rng.start is None or rng.start >= 0 else \
            timer() + rng.start
        stop = rng.stop if rng.stop is None or rng.stop >= 0 else \
            timer() + rng.stop
        out = []
        with self._lock:
            for record in self._records:
                if (start is None or start < record['finished']) and \
                   (stop is None or stop > record['started']):
                    window = record['elapsed']
                    adj_record = record.copy()
                    if start is not None and start > record['started']:
                        adj_record['started'] = start
                        window -= start - record['started']
                    if stop is not None and stop < record['finished']:
                        adj_record['finished'] = stop
                        window -= record['finished'] - stop
                    adj_record['elapsed'] = window
                    adj_record['data'] *= window / record['elapsed']
                    out.append(adj_record)
        return type(self)(out)

    def avg(self):
        with self._lock:
            done = [x for x in self._records if x['finished'] is not None]
            if not done:
                return 0
        first_start = min(x['started'] for x in done)
        last_finish = max(x['finished'] for x in done)
        walltime = last_finish - first_start
        return sum(x['data'] for x in done) / walltime

    def overlap(self):
        with self._lock:
            if not self._records:
                return 0
            now = timer()
            active = [(x['started'], x['finished'], x['elapsed'])
                      if x['finished'] is not None else
                      (x['started'], now, now - x['started'])
                      for x in self._records]
            first_start = min(x[0] for x in active)
            last_finish = max(x[1] for x in active)
            walltime = last_finish - first_start
            return sum(x[2] for x in active) / walltime


class SnowflakeChunkDownloader(object):
    u"""
    Large Result set chunk downloader class.
    """

    _sched_active_limit = SCHED_ACTIVE_LIMIT
    _sched_ready_pct_limit = SCHED_READY_PCT_LIMIT
    _sched_rate_window_factor = SCHED_RATE_WINDOW_FACTOR
    _sched_growth_factor = SCHED_GROWTH_FACTOR
    _sched_growth_min = SCHED_GROWTH_MIN
    _sched_rate_floor_pct = SCHED_RATE_FLOOR_PCT
    _sched_concur_ceil_pct = SCHED_CONCUR_CEIL_PCT

    def __init__(self, chunks, connection, cursor, qrmk, chunk_headers,
                 use_ijson=False):
        self._use_ijson = use_ijson
        self._connection = connection
        self._cursor = cursor
        self._qrmk = qrmk
        self._headers = chunk_headers
        self._manifests = chunks
        self._total = len(chunks)
        self._calling_thread = None
        self._consumed = 0
        self._sched_lock = threading.RLock()
        self._sched_work = {}
        self._sched_cursor = 0
        self._sched_active = 0
        self._sched_active_ideal = SCHED_ACTIVE_IDEAL_INITIAL
        self._sched_ready = 0
        self._stats = DownloaderStats()
        self._stats_hist = []

    def __iter__(self):
        return self

    def next(self):
        self.assertFixedThread()
        idx = self._consumed
        if idx >= self._total:
            raise StopIteration()
        with self._sched_lock:
            if idx == self._sched_cursor:
                logger.warning(u'chunk downloader reached starvation')
                self.sched_next()
        for attempt in range(MAX_RETRY_DOWNLOAD + 1):
            if attempt:
                logger.warning(u'retrying chunk %d download (retry %d/%d)',
                               idx + 1, attempt, MAX_RETRY_DOWNLOAD)
                self._sched(idx, retry=attempt)
            with self._sched_lock:
                fut = self._sched_work.pop(idx)
            # Wait for the result with a small inner timeout that is used
            # to maintain the scheduler (maybe_sched_more).  Otherwise it
            # might become starved for some network conditions.
            start_ts = timer()
            while True:
                try:
                    rows = fut.result(timeout=0.500)
                except futures.TimeoutError:
                    elapsed = timer() - start_ts
                    if elapsed > WAIT_TIME_IN_SECONDS:
                        logger.warning(
                            u'chunk %d download timed out after %g second(s)',
                            idx + 1, elapsed)
                        with self._sched_lock:
                            fut.cancel()
                            self._sched_active -= 1
                            break
                    else:
                        self._sched_tick()
                else:
                    self._consumed += 1
                    with self._sched_lock:
                        self._sched_ready -= 1
                    self._sched_tick()
                    return rows
        Error.errorhandler_wrapper(
            self._connection,
            self._cursor,
            OperationalError,
            {
                u'msg': u'The result set chunk download fails or hang for '
                        u'unknown reason.',
                u'errno': ER_CHUNK_DOWNLOAD_FAILED
            })

    __next__ = next

    def assertFixedThread(self):
        """
        Ensure threadsafety == 2 is enforced.
        https://www.python.org/dev/peps/pep-0249/#threadsafety
        """
        current = threading.current_thread()
        if self._calling_thread is None:
            self._calling_thread = weakref.ref(current)
        else:
            expected = self._calling_thread()
            assert current is expected, '%r is not %r' % (current, expected)

    def sched_next(self):
        with self._sched_lock:
            idx = self._sched_cursor
            if idx >= self._total:
                return None
            self._sched_chunk(idx)
            self._sched_cursor += 1
            return idx

    def _sched_chunk(self, idx, retry=None):
        """
        Schedule a download in a background thread.  Return a Future object
        that represents the eventual result.
        """
        future = futures.Future()
        with self._sched_lock:
            assert idx not in self._sched_work or \
                   self._sched_work[idx].cancelled()
            self._sched_work[idx] = future
        tname = 'ChunkDownloader_%d' % (idx + 1)
        if retry is not None:
            tname += '_retry_%d' % retry
        t = threading.Thread(name=tname,
                             target=self._fetch_chunk_worker_runner,
                             args=(future, self._manifests[idx]))
        t.daemon = True
        with self._sched_lock:
            self._sched_active += 1
        t.start()
        return future

    def _fetch_chunk_worker_runner(self, future, chunk):
        """
        Entry point for ChunkDownloader threads.  Thread safety rules apply
        from here out.
        """
        try:
            with self._stats.track() as data_out:
                rows = self._fetch_chunk_worker(chunk)
                data_out.append(chunk['compressedSize'])  # XXX always compressed?
        except BaseException as e:
            exc = e
        else:
            exc = None
        with self._sched_lock:
            if future.cancelled():
                if exc is None:
                    logger.warning("Ignoring good result from cancelled work")
                return
            self._sched_active -= 1
            self._sched_ready += 1
            if exc is not None:
                future.set_exception(exc)
            else:
                future.set_result(rows)
        self._sched_tick()

    def _sched_tick(self):
        """ Threadsafe scheduler for chunk queue management.  This is a variant
        of a hillclimbing algo that tries to hone in on the optimal amount of
        network concurrency.  Each "tick" monitors progress, makes any changes
        to the amount of maximum concurrency and may start new chunk
        downloads. """
        with self._sched_lock:
            if self._sched_cursor >= self._total:
                return
            rate_window = int(math.ceil(self._sched_active_ideal *
                                        self._sched_rate_window_factor))
            if len(self._stats) >= rate_window:
                rates = self._stats[-rate_window:]
                rate = rates.avg()
                concur = rates.overlap()
                sample = rate, concur
            else:
                # Connection is too immature to get a sample from.
                sample = None
            if sample is not None and \
               (not self._stats_hist or self._stats_hist[-1] != sample):
                self._stats_hist.append(sample)
                best_rate, best_concur = max(self._stats_hist)
                good_rate = self._sched_rate_floor_pct * best_rate
                good_concur = min(c for rate, c in self._stats_hist
                                  if rate >= good_rate)
                peak_concur = max(x[1] for x in self._stats_hist)
                logger.info("<i>C    PEAK:<red>%10f</red>  - BEST:<blue>%10f (%4d)</blue>   - "
                            "GOOD:<green>%10f (%4d)</green>  - CURRENT:<cyan>%10f (%4d)",
                            peak_concur, best_concur, best_rate // 2**17, good_concur,
                            good_rate // 2**17, concur, rate // 2**17)
                if best_concur > (peak_concur * self._sched_concur_ceil_pct):
                    # We have not explored the upper limits enough.
                    ideal = peak_concur + max(peak_concur *
                                              self._sched_growth_factor,
                                              self._sched_growth_min)
                    logger.info("<b><cyan>PUSH UP2! %f", ideal)
                else:
                    ideal = max(1, random.randint(int(round(good_concur)),
                                                  int(round(best_concur))))
                    logger.info("<b><yellow>HOLD2 %f", ideal)
                self._sched_active_ideal = min(self._sched_active_limit,
                                               max(1, ideal))
            if self._sched_ready <= max(1, self._sched_active_ideal * self._sched_ready_pct_limit):
                add = int(round(self._sched_active_ideal)) - self._sched_active
                for i in range(add):
                    logger.info(u'<bgyellow><black>scheduling <u>%d</u>: active: %d, ready: %d, '
                                u'limit: %f', i, self._sched_active,
                                self._sched_ready, self._sched_active_ideal)
                    self.sched_next()

    def _fetch_chunk_worker(self, chunk):
        """
        Thread worker to fetch the chunk from S3.
        """
        if self._headers is not None:
            headers = self._headers
        else:
            headers = {}
            if self._qrmk is not None:
                headers[SSE_C_ALGORITHM] = SSE_C_AES
                headers[SSE_C_KEY] = self._qrmk
        timeouts = (
            15,  # how long to wait for initial connection
            5  # how long to wait for initial request data
        )
        rest = self._connection.rest
        proxies = set_proxies(rest._proxy_host, rest._proxy_port,
                              rest._proxy_user, rest._proxy_password)
        with rest._use_requests_session() as session:
            resp = session.get(chunk['url'], proxies=proxies, headers=headers,
                               timeout=timeouts, verify=True)
        resp.raise_for_status()
        return json.loads('[%s]' % resp.content.decode())

    def terminate(self):
        """
        Terminates downloading the chunks.
        """
        with self._sched_lock:
            futures = list(self._sched_work.values())
            self._sched_work = None
        for f in futures:
            f.cancel()

    def __del__(self):
        try:
            self.terminate()
        except:
            pass
